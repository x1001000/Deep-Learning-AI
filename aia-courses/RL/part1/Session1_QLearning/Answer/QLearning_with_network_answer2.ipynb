{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡化版的deep q learning： 僅把q-table換成類神經網路\n",
    "＃在Reinforcement Learning中我們都利用Q_target（把它當作真實的Q值)來更新神經網路的weights。\n",
    "公式： Q_target = Q(s) + alpha*( R(s,a) + Q(s_)*Gamma-Q(s) )  \n",
    "(s_ 代表下一步的狀態，下一步的狀態有很多種可能，我們這裡選擇的s_是能得到最大Q的狀態，這種方法是比較agressive的方法，還有另外一種是SARSA有興趣可以自尋搜尋一下； alpha這邊我們設定為1）\n",
    "因此公式就變成 Q_target = R(s,a) + max(Q(s_,a))*Gamma  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, #動作的維度，例如上下左右就有四維\n",
    "        n_states, #用來描述狀態的維度，例如馬力歐在平面上就是二維\n",
    "        gamma = 0.9, #遠見程度\n",
    "        epsilon = 0.9,  #保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        learning_rate = 0.001 #神經網路的更新率\n",
    "    ):\n",
    "    \n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() #宣告session\n",
    "        #輸入current state\n",
    "        self.state_input = tf.placeholder(shape = [None, self.n_states], \n",
    "                                          name = 'input',\n",
    "                                          dtype = tf.float32)\n",
    "        #q_target = R(s, action) + Q(s_)*Gamma \n",
    "        self.q_target = tf.placeholder(shape = [None, self.n_actions], \n",
    "                                       name = 'q_target',\n",
    "                                       dtype = tf.float32)\n",
    "        #搭建神經網路\n",
    "        with tf.variable_scope('Q_table'):\n",
    "            self.q_eval = self.build_network('net_eval') \n",
    "        \n",
    "        # 管理神經網路的parameters\n",
    "        self.Qnet_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q_table/net_eval')\n",
    "        \n",
    "        #計算q_target和q_eval的mse來更新神經網路的參數\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Qnet_eval_params)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer()) #將神經網路初始化\n",
    "        \n",
    "            \n",
    "    def build_network(self, scope): \n",
    "        with tf.variable_scope(scope):\n",
    "            x_h1 = tf.layers.dense(inputs = self.state_input, units = 5, activation = tf.nn.tanh)        \n",
    "            x_h2 = tf.layers.dense(inputs = x_h1, units = 5, activation = tf.nn.tanh)             \n",
    "        return tf.layers.dense(inputs = x_h2, units = self.n_actions) #輸出‘不同動作’對應的Q值 \n",
    "               \n",
    " \n",
    "            \n",
    "    def choose_action(self, current_state):\n",
    "        \"\"\"\n",
    "        利用epsilon來控制探索的隨機程度，通常探索初期會給比較小的epsilon增加行為的隨機程度，\n",
    "        然後隨著遊戲的進行慢慢增加epsilon。不過由於這裡的遊戲較簡單，就不做此設定。\n",
    "        \"\"\"\n",
    "        if np.random.uniform() < self.epsilon: \n",
    "            #選擇產生估計Q值較大的行動\n",
    "            q_eval = self.sess.run(self.q_eval, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "            self.action = np.argmax(q_eval)\n",
    "        else:\n",
    "            #採取隨機行動\n",
    "            self.action = np.random.randint(0, self.n_actions)\n",
    "        return self.action\n",
    "    \n",
    "    def learn(self, current_state, reward, next_state):\n",
    "        \n",
    "        #算出實際q值並用此更新神經網路參數\n",
    "        q_eval = self.sess.run(self.q_eval, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "        q_eval_next = self.sess.run(self.q_eval, feed_dict={self.state_input: next_state[np.newaxis, :]})\n",
    "        q_target = q_eval.copy()\n",
    "        q_target[:, self.action] = reward + self.gamma*q_eval_next.max()\n",
    "        _, self.cost = self.sess.run([self.train, self.loss], feed_dict={self.state_input: current_state[np.newaxis, :],\n",
    "                                                                            self.q_target: q_target})\n",
    "    def model_save(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提示\n",
    "1. 在/Users/Yourname/anaconda3/lib/python3.6/site-packages/gym/envs底下可以找到Gym AI底下所有遊戲的文件，其中__init__.py定義了呼叫各個遊戲的名稱，例如moutain car你就得用gym.make(‘MountainCar-v0’)，另外和遊戲相關的py檔在envs/classic_control的資料夾內。我們接下來要玩的是離散版本的不是連續版的喔～，另外如果您找不到的話我們也將檔案拉出來放在gym document供大家參考。\n",
    "\n",
    "2. 在/Users/Yourname/anaconda3/lib/python3.6/site-packages/gym/envs底下可以找到Gym AI底下所有遊戲的文件，其中__init__.py定義了呼叫各個遊戲的名稱，例如moutain car你就得用gym.make(‘MountainCar-v0’)，另外和遊戲相關的py檔在envs/classic_control的資料夾內。我們接下來要玩的是離散版本的不是連續版的喔～，另外如果您找不到的話我們也將檔案拉出來放在gym document供大家參考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode0 Total Step:25918 Total Reward:-21167.473805910824\n",
      "Episode1 Total Step:24770 Total Reward:-20165.215424436952\n",
      "Episode2 Total Step:22819 Total Reward:-17925.791477029914\n",
      "Episode3 Total Step:18366 Total Reward:-14549.468423594855\n",
      "Episode4 Total Step:19822 Total Reward:-16309.28179291167\n",
      "Episode5 Total Step:22232 Total Reward:-17372.687209061103\n",
      "Episode6 Total Step:18697 Total Reward:-13751.687729503503\n",
      "Episode7 Total Step:22556 Total Reward:-18434.74256329547\n",
      "Episode8 Total Step:18686 Total Reward:-15042.218447951434\n",
      "Episode9 Total Step:20863 Total Reward:-16868.16821927978\n",
      "Episode10 Total Step:20128 Total Reward:-16421.6224800781\n",
      "Episode11 Total Step:23366 Total Reward:-19130.365145711996\n",
      "Episode12 Total Step:23997 Total Reward:-19225.82150423933\n",
      "Episode13 Total Step:16179 Total Reward:-12031.246719531173\n",
      "Episode14 Total Step:20246 Total Reward:-16062.964810347185\n",
      "Episode15 Total Step:24704 Total Reward:-19954.269132214187\n",
      "Episode16 Total Step:20063 Total Reward:-16521.535116319996\n",
      "Episode17 Total Step:26531 Total Reward:-22101.51526432143\n",
      "Episode18 Total Step:31578 Total Reward:-25612.983481549938\n",
      "Episode19 Total Step:23041 Total Reward:-18699.925288015147\n",
      "Episode20 Total Step:19149 Total Reward:-15701.575876414472\n",
      "Episode21 Total Step:18006 Total Reward:-14261.869182808829\n",
      "Episode22 Total Step:20850 Total Reward:-16965.077938515926\n",
      "Episode23 Total Step:28255 Total Reward:-23057.772645840716\n",
      "Episode24 Total Step:21308 Total Reward:-16561.470776445512\n",
      "Episode25 Total Step:22457 Total Reward:-17735.10560625221\n",
      "Episode26 Total Step:27606 Total Reward:-22012.882889445053\n",
      "Episode27 Total Step:26460 Total Reward:-22113.256537865724\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\"\"\"\n",
    "執行gym ai的遊戲時也請加下面這兩行\n",
    "\"\"\"\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "RL = QLearning(n_actions = 3, \n",
    "               n_states = 2,\n",
    "               gamma = 0.99,\n",
    "               epsilon = 0.9,\n",
    "               learning_rate = 0.01\n",
    "               )\n",
    "reward_record = []\n",
    "step_record = []\n",
    "for episode in range(100):\n",
    "    # initial environment並給出起始的state\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    current_state = env.reset()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # 產生環境視窗\n",
    "        env.render()\n",
    "        # RL choose action based on current state\n",
    "        action = RL.choose_action(current_state)\n",
    "\n",
    "        \"\"\"\n",
    "        Gym ai 的遊戲step都會output 4個值，分別為下一狀態、\n",
    "        獎勵、回合結束與否和info，不過info我們用不到因此不用管\n",
    "        他\n",
    "\n",
    "        \"\"\"\n",
    "        # RL take action and get next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        position, velocity = next_state\n",
    "        reward = abs(position + 0.5) + reward\n",
    "\n",
    "        total_reward+= reward\n",
    "\n",
    "        RL.learn(current_state, reward, next_state)\n",
    "        \n",
    "        step+=1\n",
    "        # break while loop when end of this episode\n",
    "        if done:\n",
    "            print('Episode{} Total Step:{} Total Reward:{}'.format(episode, step, total_reward))\n",
    "     \n",
    "            reward_record.append(total_reward)\n",
    "            step_record.append(step)\n",
    "            break\n",
    "        # swap state\n",
    "        current_state = next_state  \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 玩個幾回就發現total step都沒下降，不玩了～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_result = pd.concat(reward_record)\n",
    "reward_result.columns = ['Q_Agent']\n",
    "step_result = pd.concat(step_record)\n",
    "step_result.columns = ['Q_Agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
