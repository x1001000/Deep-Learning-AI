{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn # 0.3.2\n",
    "import pygame # 1.9.3\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pylab as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = 2 # number of actions \n",
    "GAMMA = 0.99 # decay rate\n",
    "OBSERVE = 10000. # timesteps to observe before training\n",
    "EXPLORE = 3000000. # frames over which to anneal epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "REPLAY_MEMORY_SIZE = 50000 # number of previous transitions to remember\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "IMG_SIZE = 80\n",
    "\n",
    "model_folder = 'saved_networks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_networks():\n",
    "    # input layer\n",
    "    input_state = tf.placeholder(\"float\", [None, IMG_SIZE, IMG_SIZE, 4])\n",
    "\n",
    "    # hidden layers   \n",
    "    h_conv1 = tflearn.conv_2d(input_state, nb_filter=32, filter_size=8, strides=4, activation='relu',\n",
    "                              weights_init=tflearn.initializations.truncated_normal(stddev=0.01))\n",
    "    h_pool1 = tflearn.max_pool_2d(h_conv1, kernel_size=2, strides=2)\n",
    "\n",
    "    h_conv2 = tflearn.conv_2d(h_pool1, nb_filter=64, filter_size=4, strides=2, activation='relu',\n",
    "                              weights_init=tflearn.initializations.truncated_normal(stddev=0.01))\n",
    "    h_conv3 = tflearn.conv_2d(h_conv2, nb_filter=64, filter_size=3, strides=1, activation='relu',\n",
    "                              weights_init=tflearn.initializations.truncated_normal(stddev=0.01))\n",
    "\n",
    "    h_conv3_flatten = tflearn.flatten(h_conv3)\n",
    "    h_fc1 = tflearn.fully_connected(h_conv3_flatten, n_units=512, activation='relu',\n",
    "                                    weights_init=tflearn.initializations.truncated_normal(stddev=0.01))\n",
    "\n",
    "    # output: Q values\n",
    "    q_value = tflearn.fully_connected(incoming=h_fc1, n_units=ACTIONS)\n",
    "\n",
    "    return input_state, q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_networks(sess, folder_name='saved_networks'):\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    checkpoint = tf.train.get_checkpoint_state(folder_name)\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    img_gray = cv2.cvtColor(cv2.resize(img, (IMG_SIZE, IMG_SIZE)), cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary = cv2.threshold(img_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    return img_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose action epsilon greedy\n",
    "def choose_action(t, readout_t, epsilon):\n",
    "    a_t = np.zeros([ACTIONS])\n",
    "    action_index = 0\n",
    "    if t % FRAME_PER_ACTION == 0:\n",
    "        if random.random() <= epsilon: # random action\n",
    "            print(\"- Random Action -\")\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else: # action of the max Q value\n",
    "            action_index = np.argmax(readout_t)\n",
    "            a_t[action_index] = 1\n",
    "    else:\n",
    "        a_t[0] = 1  # do nothing\n",
    "    return a_t, action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_networks(train):\n",
    "    # init session\n",
    "    sess = tf.InteractiveSession()\n",
    "    # init q_value networks\n",
    "    s, q_values= create_networks()\n",
    "    network_params = tf.trainable_variables()\n",
    "    # init target_q_value networks\n",
    "    st, target_q_values = create_networks()\n",
    "    target_network_params = tf.trainable_variables()[len(network_params):]\n",
    "    # init replay memory for storing the previous observations\n",
    "    replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "    \n",
    "    # update target networks operation\n",
    "    update_target_network_params = [target_network_params[i].assign(network_params[i]) for i in range(len(target_network_params))]\n",
    "    # define training operation\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    input_action = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    q_eval = tf.reduce_sum(tf.multiply(q_values, input_action), reduction_indices=1) # Q Value of the input action\n",
    "    cost = tf.reduce_mean(tf.square(y - q_eval))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    # initialize a game\n",
    "    acceleration = train\n",
    "    game_state = game.GameState()\n",
    "    game_state.acceleration = acceleration\n",
    "    # cost list\n",
    "    cost_tmp = deque(maxlen = 1000) # save a cost list to adjust the step parameter C\n",
    "    cost_tmp.append(0) # to avoid np.mean nan\n",
    "    \n",
    "    # get the first state by doing nothing and preprocess the image to (IMAGE_SIZE x IMAGE_SIZE x 4)\n",
    "    action_do_nothing = np.zeros(ACTIONS)\n",
    "    action_do_nothing[0] = 1\n",
    "    x_t_colored, r_0, terminal = game_state.frame_step(action_do_nothing) # image, reward, terminal\n",
    "    x_t = preprocess_img(x_t_colored)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    saver = load_networks(sess, folder_name=model_folder)\n",
    "    t = 0\n",
    "    score = 0\n",
    "    start_time = time.time()#to compute the time to avoid ploting too frequently \n",
    "\n",
    "    if train:\n",
    "        sess.run(update_target_network_params)\n",
    "\n",
    "        # start training\n",
    "        epsilon = INITIAL_EPSILON\n",
    "        game_count = 0\n",
    "        C = 1 # update target Q network every C step\n",
    "        while 1:\n",
    "            # choose an action epsilon greedily\n",
    "            readout_t = q_values.eval(feed_dict={s : [s_t]})[0]\n",
    "            a_t, action_index = choose_action(t, readout_t, epsilon)\n",
    "    \n",
    "            # run the selected action and observe next state and reward\n",
    "            x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "            x_t1 = preprocess_img(x_t1_colored)\n",
    "            x_t1 = np.reshape(x_t1, (IMG_SIZE, IMG_SIZE, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "    \n",
    "            # store the transition in replay memory\n",
    "            replay_memory.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "    \n",
    "            # only train if done observing\n",
    "            if t > OBSERVE:\n",
    "                # sample a minibatch for training\n",
    "                minibatch = random.sample(replay_memory, BATCH_SIZE)\n",
    "                # extract the batch variables (s_t, a_t, r_t, s_t1)\n",
    "                s_j_batch = [d[0] for d in minibatch]\n",
    "                a_batch = [d[1] for d in minibatch]\n",
    "                r_batch = [d[2] for d in minibatch]\n",
    "                s_j1_batch = [d[3] for d in minibatch]\n",
    "                # compute y\n",
    "                y_batch = []\n",
    "                readout_j1_batch = target_q_values.eval(feed_dict = {st : s_j1_batch})\n",
    "                for i in range(len(minibatch)):\n",
    "                    terminal_ = minibatch[i][4]\n",
    "                    if terminal_:\n",
    "                        # y = reward\n",
    "                        y_batch.append(r_batch[i]) \n",
    "                    else:\n",
    "                        # y = reward + GAMMA * maxQ'\n",
    "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "    \n",
    "                # perform gradient step\n",
    "                train_step.run(feed_dict = {\n",
    "                    y : y_batch,\n",
    "                    input_action : a_batch,\n",
    "                    s : s_j_batch}\n",
    "                )\n",
    "                cost_tmp.append(cost.eval(feed_dict = {y:y_batch,input_action:a_batch,s:s_j_batch}))\n",
    "                \n",
    "            # update the old values\n",
    "            s_t = s_t1\n",
    "            t += 1\n",
    "            score += r_t\n",
    "            \n",
    "            # scale down epsilon\n",
    "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "                \n",
    "            # update target network params every C steps\n",
    "            # adjust C by cost_tmp\n",
    "            if t % C == 0:\n",
    "                sess.run(update_target_network_params)\n",
    "                if np.mean(cost_tmp) > 0.5:\n",
    "                    C *= 2\n",
    "                else:\n",
    "                    C = np.ceil(C/2)\n",
    "                    \n",
    "            # save every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                saver.save(sess, model_folder + '/bird-dqn', global_step = t)\n",
    "\n",
    "            # print info\n",
    "            if t <= OBSERVE:\n",
    "                state = \"observe\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "            print(f\"TIMESTEP: {t} STATE: {state}, EPSILON: {epsilon}, ACTION: {action_index}, reward: {r_t}\")\n",
    "\n",
    "            if terminal:\n",
    "                game_state.acceleration = acceleration\n",
    "                game_count += 1\n",
    "                if time.time() - start_time > 60: # print info every 60 secs\n",
    "                    print(f'game: {game_count}, state: {state}, train_step: {t} score: {score}')\n",
    "                    print(f'C: {C}, mean cost: {np.mean(cost_tmp)}')\n",
    "                    start_time = time.time()\n",
    "                score=0\n",
    "\n",
    "    # test network\n",
    "    else:\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            readout_t = q_values.eval(feed_dict={s : [s_t]})[0]\n",
    "            a_t = np.zeros([ACTIONS])\n",
    "            if t % FRAME_PER_ACTION == 0:\n",
    "                action_index = np.argmax(readout_t)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                a_t[0] = 1 # do nothing\n",
    "\n",
    "            # run the selected action and observe next state and reward\n",
    "            x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "            x_t1 = preprocess_img(x_t1_colored)\n",
    "            x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "            s_t = s_t1\n",
    "            \n",
    "            score += r_t if r_t == 1 else 0\n",
    "            if time.time() - start_time > 10:\n",
    "                print('current score:',score)\n",
    "                start_time = time.time()\n",
    "            if terminal:\n",
    "                print('total score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "# train_networks(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "model_folder = 'pretrained'\n",
    "train_networks(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
