{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"pyglet請將版本設定成 1.2.4\n",
    "by using pip install pyglet==1.2.4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "os.sys.path.append('environment/') #學習環境的py檔在environment的資料夾中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡化版的deep q learning： 僅把q-table換成類神經網路\n",
    "＃在Reinforcement Learning中我們都利用Q_target（把它當作真實的Q值)來更新神經網路的weights。\n",
    "公式： Q_target = Q(s) + alpha*( R(s,a) + Q(s_)*Gamma-Q(s) )  \n",
    "(s_ 代表下一步的狀態，下一步的狀態有很多種可能，我們這裡選擇的s_是能得到最大Q的狀態，這種方法是比較agressive的方法，還有另外一種是SARSA有興趣可以自尋搜尋一下； alpha這邊我們設定為1）\n",
    "因此公式就變成 Q_target = R(s,a) + max(Q(s_,a))*Gamma  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, #動作的維度，例如上下左右就有四維\n",
    "        n_states, #用來描述狀態的維度，例如馬力歐在平面上就是二維\n",
    "        gamma = 0.9, #遠見程度\n",
    "        epsilon = 0.9,  #保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        learning_rate = 0.001 #神經網路的更新率\n",
    "    ):\n",
    "    \n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() #宣告session\n",
    "        #輸入current state\n",
    "        self.state_input = tf.placeholder(shape = [None, self.n_states], \n",
    "                                          name = 'input',\n",
    "                                          dtype = tf.float32)\n",
    "        #q_target = R(s, action) + Q(s_)*Gamma \n",
    "        self.q_target = tf.placeholder(shape = [None, self.n_actions], \n",
    "                                       name = 'q_target',\n",
    "                                       dtype = tf.float32)\n",
    "        #搭建神經網路\n",
    "        with tf.variable_scope('Q_table'):\n",
    "            self.q_eval = self.build_network('net_eval') \n",
    "        \n",
    "        # 管理神經網路的parameters\n",
    "        self.Qnet_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Q_table/net_eval')\n",
    "        \n",
    "        #計算q_target和q_eval的mse來更新神經網路的參數\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Qnet_eval_params)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer()) #將神經網路初始化\n",
    "        \n",
    "            \n",
    "    def build_network(self, scope): \n",
    "        with tf.variable_scope(scope):\n",
    "            x_h1 = tf.layers.dense(inputs = self.state_input, units = 5, activation = tf.nn.tanh)        \n",
    "            x_h2 = tf.layers.dense(inputs = x_h1, units = 5, activation = tf.nn.tanh)             \n",
    "        return tf.layers.dense(inputs = x_h2, units = self.n_actions) #輸出‘不同動作’對應的Q值 \n",
    "               \n",
    " \n",
    "            \n",
    "    def choose_action(self, current_state):\n",
    "        if np.random.uniform() < self.epsilon: \n",
    "            #選擇產生估計Q值較大的行動\n",
    "            q_eval = self.sess.run(self.q_eval, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "            self.action = np.argmax(q_eval)\n",
    "        else:\n",
    "            #採取隨機行動\n",
    "            self.action = np.random.randint(0, self.n_actions)\n",
    "        return self.action\n",
    "    \n",
    "    def learn(self, current_state, reward, next_state):\n",
    "        \n",
    "        #算出實際q值並用此更新神經網路參數\n",
    "        q_eval = self.sess.run(self.q_eval, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "        q_eval_next = self.sess.run(self.q_eval, feed_dict={self.state_input: next_state[np.newaxis, :]})\n",
    "        q_target = q_eval.copy()\n",
    "        q_target[:, self.action] = reward + self.gamma*q_eval_next.max()\n",
    "        _, self.cost = self.sess.run([self.train, self.loss], feed_dict={self.state_input: current_state[np.newaxis, :],\n",
    "                                                                            self.q_target: q_target})\n",
    "    def model_save(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 調整gamma並觀察學習情況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-387488d0306b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#initial environment並給出起始的state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#產生環境視窗如果使用hub則不需要\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import Mario2\n",
    "#建立環境==> env\n",
    "\n",
    "#建立RL Agent==> RL\n",
    "\n",
    "reward_record = [] #利用reward_record記錄每一回合的reward加總\n",
    "\n",
    "for episode in range(50): #每次遊戲玩50回合\n",
    "    \n",
    "    total_reward = 0\n",
    "    #initial environment並給出起始的state\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        #產生環境視窗如果使用hub則不需要  \n",
    "\n",
    "   \n",
    "               \n",
    "        #當環境給出reward後累積此回合加總\n",
    "        total_reward+= reward\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            #回合結束時記錄total_reward\n",
    "            reward_record.append(total_reward)\n",
    "            print('episode{}: total reward={}'.format(episode, total_reward))\n",
    "            break\n",
    "        #轉換到下一個state  \n",
    "\n",
    "        \n",
    "env.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_result = pd.DataFrame(reward_record)\n",
    "reward_result.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
