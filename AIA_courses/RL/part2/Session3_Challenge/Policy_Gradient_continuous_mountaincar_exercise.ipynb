{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立Policy Gradient模型\n",
    "這個是原本玩discrete遊戲的policy gradient，請依照投影片提示更改class的內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Gradient:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        n_states, \n",
    "        gamma = 0.9, #遠見程度\n",
    "        epsilon = None,  #保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        epsilon_increase = None,\n",
    "        learning_rate = 0.001, #神經網路的更新率\n",
    "        #memory_size = 50, #####\n",
    "        #batch_size = 32, #####\n",
    "        nueron_num = 10\n",
    "    ):\n",
    "    \n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        #self.epsilon_max = epsilon #####\n",
    "        #self.epsilon_increase = epsilon_increase #####\n",
    "        #self.epsilon = 0 if epsilon_increase is not None else epsilon #####\n",
    "        self.lr = learning_rate\n",
    "        #self.memory_size = memory_size #####\n",
    "        #self.memory_counter = 0 #####\n",
    "        #self.batch_size = batch_size ####\n",
    "        self.nueron_num = nueron_num\n",
    "        \n",
    "        ##### initialize memory\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.past_state, self.past_action, self.past_reward = [], [], []\n",
    "        self.action_one_hot = np.zeros(self.n_actions, dtype=np.int32)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() #宣告session\n",
    "        #輸入current state\n",
    "        self.state_input = tf.placeholder(shape = [None, self.n_states], \n",
    "                                          name = 'state_input',\n",
    "                                          dtype = tf.float32)\n",
    "        \"\"\"\n",
    "        輸入real action和神經網路的output act_proba算cross entropy當作更新方向\n",
    "        以超級瑪莉的遊戲為例 action = [上, 下, 左, 右] 如果實際action為向左則\n",
    "        action = [0, 0, 1, 0]。\n",
    "        也可以將這四個動作用0, 1, 2 ,3代表，如此的話只需要用一維來存取動作，也就是輸\n",
    "        入shape = [None, 1]，那後面再算cross entropy的話就要用tf.nn.sparse_\n",
    "        softmax_cross_entropy_with_logits，大家也可以試著改寫看看。\n",
    "        \"\"\"    \n",
    "        self.real_action = tf.placeholder(shape = [None, self.n_actions], \n",
    "                                          name = 'real_action',\n",
    "                                          dtype = tf.float32)\n",
    "        \"\"\"\n",
    "        但是有時候產生的動作會帶來好的效果或壞的效果並且程度不一，因此loss不能光用神經網路的\n",
    "        輸出action_proba和real action的cross entropy代表，因此這邊乘上Vt來校正loss。\n",
    "        例如某個動作很有幫助那必然會產生很大的action_reward，因此乘上很大的action_reward\n",
    "        即可加大loss讓此動作之後產生的機率被放大；相反的，某個動作如果產生很好的效果反而會帶\n",
    "        來負的action_reward使得loss變負的，讓更新方向相反使得之後輸出此動作的機會減少。\n",
    "        \"\"\"\n",
    "        self.Vt = tf.placeholder(shape= [None, ], \n",
    "                                            name=\"Vt\",\n",
    "                                            dtype = tf.float32)\n",
    "        #搭建神經網路\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.act_proba = self.build_network(self.nueron_num, Trainable = True, \\\n",
    "                             scope = 'net_eval') \n",
    "            \n",
    "        \n",
    "        #管理神經網路的parameters\n",
    "        self.Actor_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/net_eval')\n",
    "        \n",
    "        \n",
    "        #loss\n",
    "        \"\"\"\n",
    "        算出 “神經網路輸出的動作機率”與 “實際動作”的cross entropy當作loss，但是更新的方向和力道就利用action\n",
    "        reward來決定。例如，這一回合產生的所有動作組合如果得到很好的reward，那就應該讓神經網路的輸出機率更靠近實\n",
    "        際輸出的結果，因此cross_entropy和action_reward相乘的到的loss就更大，更新力度就更大。相反的，這一回\n",
    "        合產生的所有動作組合如果得到負的reward，那就應該讓神經網路輸出動作的機率更遠離實際輸出結果，在這樣的狀況\n",
    "        下，cross_entropy和action_reward相乘的到的loss就會得到負的，神經網路的參數更新方向就會往反方向。\n",
    "        \"\"\"\n",
    "        self.cross_entropy = tf.reduce_sum(-tf.log(self.act_proba)*self.real_action, axis=1)\n",
    "        self.loss = tf.reduce_sum(self.cross_entropy*self.Vt)\n",
    " \n",
    "        \n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Actor_eval_params)\n",
    "    \n",
    "        self.sess.run(tf.global_variables_initializer()) #將神經網路初始化\n",
    "    \n",
    "    def write_memory(self, current_state, reward, action): #####\n",
    "        \n",
    "        action_one_hot = self.action_one_hot.copy()\n",
    "        action_one_hot[action] = 1\n",
    "        self.past_state.append(current_state)\n",
    "        self.past_action.append(action_one_hot)\n",
    "        self.past_reward.append(reward)\n",
    "    \n",
    "    \n",
    "    def build_network(self, neuron_num, Trainable, scope): \n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.random_normal_initializer(0., 0.3)\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "            \n",
    "            x_h1 = tf.layers.dense(inputs = self.state_input, units = neuron_num, \\\n",
    "                   activation = tf.nn.tanh, kernel_initializer=init_w, \\\n",
    "                   bias_initializer=init_b, trainable=Trainable) \n",
    "            x_h2 = tf.layers.dense(inputs = self.state_input, units = neuron_num, \\\n",
    "                   activation = tf.nn.tanh, kernel_initializer=init_w, \\\n",
    "                   bias_initializer=init_b, trainable=Trainable)\n",
    "            \n",
    "            output = tf.layers.dense(inputs = x_h2, units = self.n_actions, \\\n",
    "                   activation = tf.nn.softmax, kernel_initializer=init_w, \\\n",
    "                   bias_initializer=init_b, trainable=Trainable)#輸出為個行為的機率，因此使用softmax\n",
    "            \n",
    "        \n",
    "        return output #輸出‘不同動作’對應的Q值 \n",
    "               \n",
    " \n",
    "            \n",
    "    def choose_action(self, current_state):\n",
    "        \n",
    "        act_proba = self.sess.run(self.act_proba, feed_dict={self.state_input: current_state[np.newaxis, :]})\n",
    "        \"\"\"\n",
    "        根據神經網路輸出個動作的機率當作權重做隨機抽樣選出動作。一樣舉超級瑪莉的例子，如果輸出\n",
    "        [上, 下, 左, 右]的機率分別為[0.3, 0.1, 0.2, 0.4]，那做隨機抽樣就分別有30%, 10%, \n",
    "        20%和40%的機率抽到上、下、左、右，因為這樣的做法已具有隨機性因此不需要用epsilon來控制\n",
    "        行為的隨機程度來增加探索。\n",
    "        \"\"\"\n",
    "        self.action = np.random.choice(range(act_proba.shape[1]), p=act_proba.ravel())  \n",
    "   \n",
    "        return self.action\n",
    "    \n",
    "    def learn(self): #####\n",
    "        \n",
    "        #用reward算出更新向量\n",
    "        Vt = self.calculate_Vt(self.past_reward)\n",
    "        #print(np.array(self.past_action).shape)\n",
    "        #print(np.array(self.past_action))\n",
    "        #將整個episode的資料放進來做更新\n",
    "        self.sess.run(self.train, feed_dict={\n",
    "             self.state_input: np.vstack(self.past_state),  # shape=[None, n_state]\n",
    "             self.real_action: np.array(self.past_action),  # shape=[None, n_actions]\n",
    "             self.Vt: np.array(Vt)  # shape=[None, ]\n",
    "        })\n",
    "        #更新完後將記憶庫清空\n",
    "        self.past_state, self.past_action, self.past_reward = [], [], []    \n",
    "    \n",
    "    def calculate_Vt(self, reward):\n",
    "        # discount episode rewards\n",
    "        Vt = np.zeros_like(reward, dtype=np.float64)\n",
    "        Vt_temp = 0\n",
    "        for t in reversed(range(0, len(reward))):\n",
    "            Vt_temp = reward[t] + Vt_temp * self.gamma\n",
    "            Vt[t] = Vt_temp\n",
    "\n",
    "        # normalize episode rewards\n",
    "        \n",
    "        Vt -= np.mean(Vt)\n",
    "        Vt /= np.std(Vt)\n",
    "        \n",
    "        return Vt    \n",
    "        \n",
    "    def model_save(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/Jacklin/anaconda3/lib/python3.6/site-packages/')\n",
    "import gym\n",
    "\n",
    "def training(save_model, model_name):\n",
    "    step_record = []\n",
    "    reward_record = []\n",
    "    for episode in range(200):\n",
    "        # initial environment並給出起始的state\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            # 產生環境視窗\n",
    "            env.render()\n",
    "\n",
    "            # 根據現在的狀態選擇動作\n",
    "            action = RL.choose_action(current_state)\n",
    "\n",
    "            # 產生動作和環境互動後產生下一個狀態、獎勵值及遊戲是否結束\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward+= reward\n",
    "            \n",
    "            #這句可加可不加，只是怕因為machine變太強會玩很久不死，所以強迫結束遊戲\n",
    "            if step > 10000:\n",
    "                done = True\n",
    "            \n",
    "            # 將資訊存至記憶體中以便進行experience replay\n",
    "            RL.write_memory(current_state, reward, action)\n",
    "            \n",
    "\n",
    "            # swap state\n",
    "            current_state = next_state\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                RL.learn()\n",
    "                print('episode:{} steps:{} total reward:{}'.format(episode, step, total_reward))\n",
    "                step_record.append(step)\n",
    "                reward_record.append(total_reward)\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "    # end of game\n",
    "    if save_model:\n",
    "        RL.model_save(model_name)\n",
    "    print('game over')\n",
    "    env.close()\n",
    "    return step_record, reward_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "RL = Policy_Gradient(n_actions = 1, \n",
    "                   n_states = 2,\n",
    "                   gamma = 0.99,\n",
    "                   learning_rate = 0.01,\n",
    "                 )\n",
    "step_record, reward_record = training(save_model = True, model_name='PG_MountainCarContinuous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_result = pd.DataFrame(reward_record)\n",
    "reward_result.columns = ['Policy Gradient']\n",
    "reward_result.plot()\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
