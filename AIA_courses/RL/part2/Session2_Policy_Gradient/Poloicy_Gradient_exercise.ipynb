{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建力Policy Gradient模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Gradient:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        n_states, \n",
    "        gamma = 0.9, #遠見程度\n",
    "        epsilon = None,  #保守程度，越大就越容易用Q值大小來採取行動；越小則越容易產生隨機行動\n",
    "        epsilon_increase = None,\n",
    "        learning_rate = 0.001, #神經網路的更新率\n",
    "        #memory_size = 50, #####\n",
    "        #batch_size = 32, #####\n",
    "        nueron_num = 10\n",
    "    ):\n",
    "    \n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.gamma = gamma\n",
    "        #self.epsilon_max = epsilon #####\n",
    "        #self.epsilon_increase = epsilon_increase #####\n",
    "        #self.epsilon = 0 if epsilon_increase is not None else epsilon #####\n",
    "        self.lr = learning_rate\n",
    "        #self.memory_size = memory_size #####\n",
    "        #self.memory_counter = 0 #####\n",
    "        #self.batch_size = batch_size ####\n",
    "        self.nueron_num = nueron_num\n",
    "        \n",
    "        ##### initialize memory\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.past_state, self.past_action, self.past_reward = [], [], []\n",
    "        self.action_one_hot = np.zeros(self.n_actions, dtype=np.int32)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        tf.reset_default_graph() ## 重新 build graph 需要跑這行\n",
    "        self.sess = tf.Session() #宣告session\n",
    "        #輸入current state\n",
    "        self.state_input = tf.placeholder(shape = [None, self.n_states], \n",
    "                                          name = 'state_input',\n",
    "                                          dtype = tf.float32)\n",
    "        \"\"\"\n",
    "        輸入real action和神經網路的output act_proba算cross entropy當作更新方向\n",
    "        以超級瑪莉的遊戲為例 action = [上, 下, 左, 右] 如果實際action為向左則\n",
    "        action = [0, 0, 1, 0]。\n",
    "        也可以將這四個動作用0, 1, 2 ,3代表，如此的話只需要用一維來存取動作，也就是輸\n",
    "        入shape = [None, 1]，那後面再算cross entropy的話就要用tf.nn.sparse_\n",
    "        softmax_cross_entropy_with_logits，大家也可以試著改寫看看。\n",
    "        \"\"\"    \n",
    "        self.real_action = tf.placeholder(shape = [None, self.n_actions], \n",
    "                                          name = 'real_action',\n",
    "                                          dtype = tf.float32)\n",
    "        \"\"\"\n",
    "        但是有時候產生的動作會帶來好的效果或壞的效果並且程度不一，因此loss不能光用神經網路的\n",
    "        輸出action_proba和real action的cross entropy代表，因此這邊乘上action_reward\n",
    "        來校正loss。例如某個動作很有幫助那必然會產生很大的action_reward，因此乘上很大的\n",
    "        action_reward即可加大loss讓此動作之後產生的機率被放大；相反的，某個動作如果產生很\n",
    "        好的效果反而會帶來負的action_reward使得loss變負的，讓更新方向相反使得之後輸出此動\n",
    "        作的機會減少。\n",
    "        \"\"\"\n",
    "        self.Vt = tf.placeholder(shape= [None, ], \n",
    "                                            name=\"Vt\",\n",
    "                                            dtype = tf.float32)\n",
    "        #搭建神經網路\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.act_proba = self.build_network(self.nueron_num, Trainable = True, \\\n",
    "                             scope = 'net_eval') \n",
    "            \n",
    "        \n",
    "        #管理神經網路的parameters\n",
    "        self.Actor_eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/net_eval')\n",
    "        \n",
    "        \n",
    "        #loss\n",
    "        \"\"\"\n",
    "        算出 “神經網路輸出的動作機率”與 “實際動作”的cross entropy當作loss，但是更新的方向和力道就利用action\n",
    "        reward來決定。例如，這一回合產生的所有動作組合如果得到很好的reward，那就應該讓神經網路的輸出機率更靠近實\n",
    "        際輸出的結果，因此cross_entropy和action_reward相乘的到的loss就更大，更新力度就更大。相反的，這一回\n",
    "        合產生的所有動作組合如果得到負的reward，那就應該讓神經網路輸出動作的機率更遠離實際輸出結果，在這樣的狀況\n",
    "        下，cross_entropy和action_reward相乘的到的loss就會得到負的，神經網路的參數更新方向就會往反方向。\n",
    "        \"\"\"\n",
    "        self.cross_entropy = tf.reduce_sum(-tf.log(self.act_proba)*self.real_action, axis=1)\n",
    "        self.loss = tf.reduce_sum(self.cross_entropy*self.Vt)\n",
    " \n",
    "        \n",
    "        self.train = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, var_list=self.Actor_eval_params)\n",
    "    \n",
    "        self.sess.run(tf.global_variables_initializer()) #將神經網路初始化\n",
    "    \n",
    "    def write_memory(self, current_state, reward, action): #####\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_network(self, neuron_num, Trainable, scope): \n",
    "         \n",
    "               \n",
    " \n",
    "            \n",
    "    def choose_action(self, current_state):\n",
    "        \n",
    "        \n",
    "    \n",
    "    def learn(self): #####\n",
    "        \n",
    "        \n",
    "    \n",
    "    def calculate_Vt(self):\n",
    "            \n",
    "        \n",
    "    def model_save(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, \"saved_models/{}.ckpt\".format(model_name))\n",
    "    \n",
    "    def model_restore(self, model_name):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, \"saved_models/{}.ckpt\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(save_model, model_name):\n",
    "    step_record = []\n",
    "    #dead_record = []\n",
    "    for episode in range(200):\n",
    "        # initial environment並給出起始的state\n",
    "        current_state = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            # 產生環境視窗\n",
    "            env.render()\n",
    "\n",
    "            # 根據現在的狀態選擇動作\n",
    "            action = RL.choose_action(current_state)\n",
    "\n",
    "            # 產生動作和環境互動後產生下一個狀態、獎勵值及遊戲是否結束\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            total_reward+= reward\n",
    "            \n",
    "            \n",
    "            # 將資訊存至記憶體中以便進行experience replay\n",
    "            RL.write_memory(current_state, reward, action)\n",
    "            \n",
    "            #if reward < 0 :\n",
    "            #    dead+=1\n",
    "            # swap state\n",
    "            current_state = next_state\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                RL.learn()\n",
    "                print('episode:{} steps:{} total_reward:{}'.format(episode, step, total_reward))\n",
    "                step_record.append(step)\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "    # end of game\n",
    "    if save_model:\n",
    "        RL.model_save(model_name)\n",
    "    print('game over')\n",
    "    env.close()\n",
    "    return step_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:0 steps:28 total_reward:29.0\n",
      "episode:1 steps:22 total_reward:23.0\n",
      "episode:2 steps:11 total_reward:12.0\n",
      "episode:3 steps:8 total_reward:9.0\n",
      "episode:4 steps:14 total_reward:15.0\n",
      "episode:5 steps:20 total_reward:21.0\n",
      "episode:6 steps:25 total_reward:26.0\n",
      "episode:7 steps:32 total_reward:33.0\n",
      "episode:8 steps:17 total_reward:18.0\n",
      "episode:9 steps:11 total_reward:12.0\n",
      "episode:10 steps:16 total_reward:17.0\n",
      "episode:11 steps:9 total_reward:10.0\n",
      "episode:12 steps:14 total_reward:15.0\n",
      "episode:13 steps:19 total_reward:20.0\n",
      "episode:14 steps:16 total_reward:17.0\n",
      "episode:15 steps:19 total_reward:20.0\n",
      "episode:16 steps:28 total_reward:29.0\n",
      "episode:17 steps:11 total_reward:12.0\n",
      "episode:18 steps:23 total_reward:24.0\n",
      "episode:19 steps:13 total_reward:14.0\n",
      "episode:20 steps:32 total_reward:33.0\n",
      "episode:21 steps:23 total_reward:24.0\n",
      "episode:22 steps:30 total_reward:31.0\n",
      "episode:23 steps:28 total_reward:29.0\n",
      "episode:24 steps:18 total_reward:19.0\n",
      "episode:25 steps:78 total_reward:79.0\n",
      "episode:26 steps:33 total_reward:34.0\n",
      "episode:27 steps:18 total_reward:19.0\n",
      "episode:28 steps:18 total_reward:19.0\n",
      "episode:29 steps:13 total_reward:14.0\n",
      "episode:30 steps:15 total_reward:16.0\n",
      "episode:31 steps:27 total_reward:28.0\n",
      "episode:32 steps:41 total_reward:42.0\n",
      "episode:33 steps:26 total_reward:27.0\n",
      "episode:34 steps:14 total_reward:15.0\n",
      "episode:35 steps:23 total_reward:24.0\n",
      "episode:36 steps:44 total_reward:45.0\n",
      "episode:37 steps:12 total_reward:13.0\n",
      "episode:38 steps:36 total_reward:37.0\n",
      "episode:39 steps:35 total_reward:36.0\n",
      "episode:40 steps:46 total_reward:47.0\n",
      "episode:41 steps:47 total_reward:48.0\n",
      "episode:42 steps:20 total_reward:21.0\n",
      "episode:43 steps:25 total_reward:26.0\n",
      "episode:44 steps:20 total_reward:21.0\n",
      "episode:45 steps:23 total_reward:24.0\n",
      "episode:46 steps:58 total_reward:59.0\n",
      "episode:47 steps:16 total_reward:17.0\n",
      "episode:48 steps:31 total_reward:32.0\n",
      "episode:49 steps:27 total_reward:28.0\n",
      "episode:50 steps:69 total_reward:70.0\n",
      "episode:51 steps:32 total_reward:33.0\n",
      "episode:52 steps:28 total_reward:29.0\n",
      "episode:53 steps:20 total_reward:21.0\n",
      "episode:54 steps:38 total_reward:39.0\n",
      "episode:55 steps:32 total_reward:33.0\n",
      "episode:56 steps:48 total_reward:49.0\n",
      "episode:57 steps:37 total_reward:38.0\n",
      "episode:58 steps:24 total_reward:25.0\n",
      "episode:59 steps:90 total_reward:91.0\n",
      "episode:60 steps:41 total_reward:42.0\n",
      "episode:61 steps:37 total_reward:38.0\n",
      "episode:62 steps:80 total_reward:81.0\n",
      "episode:63 steps:40 total_reward:41.0\n",
      "episode:64 steps:59 total_reward:60.0\n",
      "episode:65 steps:38 total_reward:39.0\n",
      "episode:66 steps:10 total_reward:11.0\n",
      "episode:67 steps:26 total_reward:27.0\n",
      "episode:68 steps:47 total_reward:48.0\n",
      "episode:69 steps:33 total_reward:34.0\n",
      "episode:70 steps:66 total_reward:67.0\n",
      "episode:71 steps:42 total_reward:43.0\n",
      "episode:72 steps:67 total_reward:68.0\n",
      "episode:73 steps:137 total_reward:138.0\n",
      "episode:74 steps:98 total_reward:99.0\n",
      "episode:75 steps:82 total_reward:83.0\n",
      "episode:76 steps:154 total_reward:155.0\n",
      "episode:77 steps:31 total_reward:32.0\n",
      "episode:78 steps:66 total_reward:67.0\n",
      "episode:79 steps:107 total_reward:108.0\n",
      "episode:80 steps:92 total_reward:93.0\n",
      "episode:81 steps:105 total_reward:106.0\n",
      "episode:82 steps:111 total_reward:112.0\n",
      "episode:83 steps:38 total_reward:39.0\n",
      "episode:84 steps:54 total_reward:55.0\n",
      "episode:85 steps:102 total_reward:103.0\n",
      "episode:86 steps:25 total_reward:26.0\n",
      "episode:87 steps:120 total_reward:121.0\n",
      "episode:88 steps:60 total_reward:61.0\n",
      "episode:89 steps:77 total_reward:78.0\n",
      "episode:90 steps:87 total_reward:88.0\n",
      "episode:91 steps:83 total_reward:84.0\n",
      "episode:92 steps:126 total_reward:127.0\n",
      "episode:93 steps:88 total_reward:89.0\n",
      "episode:94 steps:233 total_reward:234.0\n",
      "episode:95 steps:210 total_reward:211.0\n",
      "episode:96 steps:101 total_reward:102.0\n",
      "episode:97 steps:242 total_reward:243.0\n",
      "episode:98 steps:16 total_reward:17.0\n",
      "episode:99 steps:73 total_reward:74.0\n",
      "episode:100 steps:75 total_reward:76.0\n",
      "episode:101 steps:298 total_reward:299.0\n",
      "episode:102 steps:228 total_reward:229.0\n",
      "episode:103 steps:167 total_reward:168.0\n",
      "episode:104 steps:111 total_reward:112.0\n",
      "episode:105 steps:178 total_reward:179.0\n",
      "episode:106 steps:178 total_reward:179.0\n",
      "episode:107 steps:166 total_reward:167.0\n",
      "episode:108 steps:136 total_reward:137.0\n",
      "episode:109 steps:137 total_reward:138.0\n",
      "episode:110 steps:255 total_reward:256.0\n",
      "episode:111 steps:89 total_reward:90.0\n",
      "episode:112 steps:121 total_reward:122.0\n",
      "episode:113 steps:188 total_reward:189.0\n",
      "episode:114 steps:197 total_reward:198.0\n",
      "episode:115 steps:251 total_reward:252.0\n",
      "episode:116 steps:254 total_reward:255.0\n",
      "episode:117 steps:186 total_reward:187.0\n",
      "episode:118 steps:114 total_reward:115.0\n",
      "episode:119 steps:147 total_reward:148.0\n",
      "episode:120 steps:235 total_reward:236.0\n",
      "episode:121 steps:182 total_reward:183.0\n",
      "episode:122 steps:290 total_reward:291.0\n",
      "episode:123 steps:147 total_reward:148.0\n",
      "episode:124 steps:557 total_reward:558.0\n",
      "episode:125 steps:288 total_reward:289.0\n",
      "episode:126 steps:401 total_reward:402.0\n",
      "episode:127 steps:445 total_reward:446.0\n",
      "episode:128 steps:181 total_reward:182.0\n",
      "episode:129 steps:620 total_reward:621.0\n",
      "episode:130 steps:169 total_reward:170.0\n",
      "episode:131 steps:258 total_reward:259.0\n",
      "episode:132 steps:85 total_reward:86.0\n",
      "episode:133 steps:643 total_reward:644.0\n",
      "episode:134 steps:152 total_reward:153.0\n",
      "episode:135 steps:834 total_reward:835.0\n",
      "episode:136 steps:285 total_reward:286.0\n",
      "episode:137 steps:117 total_reward:118.0\n",
      "episode:138 steps:543 total_reward:544.0\n",
      "episode:139 steps:156 total_reward:157.0\n",
      "episode:140 steps:132 total_reward:133.0\n",
      "episode:141 steps:122 total_reward:123.0\n",
      "episode:142 steps:196 total_reward:197.0\n",
      "episode:143 steps:302 total_reward:303.0\n",
      "episode:144 steps:639 total_reward:640.0\n",
      "episode:145 steps:384 total_reward:385.0\n",
      "episode:146 steps:722 total_reward:723.0\n",
      "episode:147 steps:948 total_reward:949.0\n",
      "episode:148 steps:388 total_reward:389.0\n",
      "episode:149 steps:232 total_reward:233.0\n",
      "episode:150 steps:662 total_reward:663.0\n",
      "episode:151 steps:378 total_reward:379.0\n",
      "episode:152 steps:1649 total_reward:1650.0\n",
      "episode:153 steps:206 total_reward:207.0\n",
      "episode:154 steps:394 total_reward:395.0\n",
      "episode:155 steps:378 total_reward:379.0\n",
      "episode:156 steps:411 total_reward:412.0\n",
      "episode:157 steps:1927 total_reward:1928.0\n",
      "episode:158 steps:1658 total_reward:1659.0\n",
      "episode:159 steps:727 total_reward:728.0\n",
      "episode:160 steps:269 total_reward:270.0\n",
      "episode:161 steps:466 total_reward:467.0\n",
      "episode:162 steps:422 total_reward:423.0\n",
      "episode:163 steps:239 total_reward:240.0\n",
      "episode:164 steps:401 total_reward:402.0\n",
      "episode:165 steps:273 total_reward:274.0\n",
      "episode:166 steps:251 total_reward:252.0\n",
      "episode:167 steps:250 total_reward:251.0\n",
      "episode:168 steps:369 total_reward:370.0\n",
      "episode:169 steps:334 total_reward:335.0\n",
      "episode:170 steps:619 total_reward:620.0\n",
      "episode:171 steps:776 total_reward:777.0\n",
      "episode:172 steps:1324 total_reward:1325.0\n",
      "episode:173 steps:283 total_reward:284.0\n",
      "episode:174 steps:12260 total_reward:12261.0\n",
      "episode:175 steps:4250 total_reward:4251.0\n",
      "episode:176 steps:242 total_reward:243.0\n",
      "episode:177 steps:1298 total_reward:1299.0\n",
      "episode:178 steps:261 total_reward:262.0\n",
      "episode:179 steps:1186 total_reward:1187.0\n",
      "episode:180 steps:3590 total_reward:3591.0\n",
      "episode:181 steps:848 total_reward:849.0\n",
      "episode:182 steps:29601 total_reward:29602.0\n",
      "episode:183 steps:14910 total_reward:14911.0\n",
      "episode:184 steps:2776 total_reward:2777.0\n",
      "episode:185 steps:22343 total_reward:22344.0\n",
      "episode:186 steps:7534 total_reward:7535.0\n",
      "episode:187 steps:284 total_reward:285.0\n",
      "episode:188 steps:2618 total_reward:2619.0\n",
      "episode:189 steps:2069 total_reward:2070.0\n",
      "episode:190 steps:26098 total_reward:26099.0\n",
      "episode:191 steps:4118 total_reward:4119.0\n",
      "episode:192 steps:45199 total_reward:45200.0\n",
      "episode:193 steps:72674 total_reward:72675.0\n",
      "episode:194 steps:1255 total_reward:1256.0\n",
      "episode:195 steps:3114 total_reward:3115.0\n",
      "episode:196 steps:215068 total_reward:215069.0\n",
      "episode:197 steps:131137 total_reward:131138.0\n"
     ]
    }
   ],
   "source": [
    "step_result = []\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "RL = Policy_Gradient(n_actions = 2, \n",
    "                   n_states = 4,\n",
    "                   gamma = 0.99,\n",
    "                   learning_rate = 0.01,\n",
    "                 )\n",
    "step_record = training(save_model = True, model_name='PG_try')\n",
    "step_result.append(pd.DataFrame(data = step_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_result = pd.DataFrame(reward_record)\n",
    "reward_result.columns = ['Policy Gradient']\n",
    "reward_result.plot()\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
